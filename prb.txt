Created Date Time Select all rows for Drill Down.
Created Date Time
Sorted by Created Date Time, oldest-to-newestAscending
Problem Priority
Sorted by Problem Priority, ascending picklist orderAscending
Problem Number
Sorted by Problem Number, A-to-ZAscending
Select row for Drill Down.
8/10/2025 - 8/16/2025(1)
P2(1)
PRB-0028514(1)
Select row for Drill Down.
8/17/2025 - 8/23/2025(2)
P1(1)
PRB-0028579(1)
P2(1)
PRB-0028537(1)
Select row for Drill Down.
8/24/2025 - 8/30/2025(1)
P1(1)
PRB-0028619(1)
Select row for Drill Down.
8/31/2025 - 9/6/2025(1)
P2(1)
PRB-0028630(1)
Created Date Time
Sorted by Created Date Time, oldest-to-newestAscending
Problem Priority
Sorted by Problem Priority, ascending picklist orderAscending
Problem Number
Sorted by Problem Number, A-to-ZAscending
Team: Team Name
Cloud
Problem State
Customer Impact
External RCA Requested
Problem Description
Repeat Incident
Created Date
SLO Expected End Time
Initial Incident: Case Number
Sorted by Initial Incident: Case Number, A-to-ZAscending
Initial Work Issue: Work ID
Customer Experience
What Happened?
Proximate Cause (Why did it happen)?
How did we find out about the issue?
How was it Resolved?
Key Questions for Retro
Root Cause
Next Steps
Retro Summary
Consolidate To: Problem Number
Retrospective Completion Date
Owner: Full Name
Formula
Bring Back?
8/10/2025 - 8/16/2025(1)
P2(1)
PRB-0028514(1)
CRM Database Sustaining Engineering
SDB
Waiting 3rd Party
Performance degradation (general)
feature not included
PRB Retrospective | SEV-2 | 08/15/2025 | na248 Single Org activities causing Connection Pool Timeouts

[THIS PRB IS MANAGED BY QUIP2GUS]
feature not included
8/15/2025
9/23/2025, 6:28 AM
79319100
W-19344203
User Experience: Single org slow access to sfdc resources due to their own actions.. | Impact Quantification: Q: What #/% of users/customers/requests got impacted (aggregated across all infra units)?. A: .
[THIS PRB IS MANAGED BY QUIP2GUS]
Any updates made in GUS will be overwritten! Update Quip doc: https://salesforce.quip.com/N8dCAcKIh53d
​
na248 Single Org activities causing Connection Pool Timeouts. Pls refer to Feed Post for
Single org throttled.
https://salesforce.enterprise.slack.com/archives/C02J7CM51N3/p1755252423079589
The incident has self-resolved, and no further manual actions are required. Auto-remediation was applied through CSP throttle rules, specifically targeting the dbConnPool source, which helped alleviate the impact.
​
Q: Explain in detail the Time to
Proximate Cause: This issue was localized to a single organization (Org Id: 00D5Y000001c9oE), affecting nodes 16, 15, 1, and 8.
​
Key Questions for Retro (enter as Problem Statements below): 
​
Q: How can we make this response standard? The a
-
TBD - Fill out CARs table in Quip
Proximate Cause: This issue was localized to a single organization (Org Id: 00D5Y000001c9oE), affecting nodes 16, 15, 1, and 8.
​
Key Questions for Retro (enter as Problem Statements below): 
​
Q: How can we make this response standard? The a
-
-
Site Reliability
-
8/17/2025 - 8/23/2025(2)
P1(1)
PRB-0028579(1)
CRM DB Replication and Recovery as a Service (CRRS)
SDB
Analysis Complete
Service disruption (general)
feature not included
PRB Retrospective | SEV-1 | 08/23/2025 | DB Error/Down for Prod1

[THIS PRB IS MANAGED BY QUIP2GUS]
feature not included
8/23/2025
9/3/2025, 5:00 PM
79692959
W-19411801
User Experience: Omni Standard was disrupted. | Impact Quantification: Q: What #/% of users/customers/requests got impacted (aggregated across all infra units)?. A: .
[THIS PRB IS MANAGED BY QUIP2GUS]
Any updates made in GUS will be overwritten! Update Quip doc: https://salesforce.quip.com/ZuqkAxrE2Abc
​
Omni Standard was impacted when the LA13-CORE1 DB became unavailable to SCRT1. The incident
Database filled up due to archival
https://salesforce-internal.slack.com/archives/CDQ3U66E5/p1755977133679499
The CDSE team restored the service by clearing the archive logs, which released the space to un-freeze the database.
​
Q: Explain in detail the Time to Fix (i.e key events from detection to resolution)
A: 
​
Q: Was a known mitigation for t
Proximate Cause: The database became unresponsive because archival logs filled up the space.
​
Key Questions for Retro (enter as Problem Statements below): 
​
Q: What needs to be fixed in automation to make sure this doesnt repeat? 
A: 
1) dp-master and backup-worker services were unable to fetch secrets from vault due to missing vault label.
1) Migrate archive purging job from Heimdall to UIP to povide more visiblity.
2) Troubleshoot 3ER worker/master down issue due to terraform change.
3) Enable PagerDuty alert for archdg space usage at 70% and 80% in Hyperforce.
4) Schedule
Proximate Cause: The database became unresponsive because archival logs filled up the space.
​
Key Questions for Retro (enter as Problem Statements below): 
​
Q: What needs to be fixed in automation to make sure this doesnt repeat? 
A: 
-
-
Site Reliability
-
P2(1)
PRB-0028537(1)
CRM Database Sustaining Engineering
SDB
Waiting 3rd Party
Service disruption (general)
feature not included
PRB Retrospective | SEV-2 | 08/19/2025 | Issues with DB cluster on POD254

[THIS PRB IS MANAGED BY QUIP2GUS]
feature not included
8/19/2025
9/3/2025, 5:23 AM
79472929
W-19374544
User Experience: Service Disruption. | Impact Quantification: Q: What #/% of users/customers/requests got impacted (aggregated across all infra units)?. A: .
[THIS PRB IS MANAGED BY QUIP2GUS]
Any updates made in GUS will be overwritten! Update Quip doc: https://salesforce.quip.com/15ODAWS7JMbb
​
The SRE team received a host down alert indicating that the host on POD254 | blade7-6 was d
Database cluster went down causing the customer impact.
https://salesforce-internal.slack.com/archives/C02CP372GTE/p1755604098134379
Database team rebooted the DB blades.
​
Q: Explain in detail the Time to Fix (i.e key events from detection to resolution)
A: 
​
Q: Was a known mitigation for this incident? If yes, please describe it and can it be automated?  
A: 
​
Proximate Cause: Database cluster went down causing the customer impact.
​
Key Questions for Retro (enter as Problem Statements below): 
​
Q: 1. Tracer - Did we use Tracer? - No As of now tracer is not installed in the Commerce cloud environm
-
TBD - Fill out CARs table in Quip
Proximate Cause: Database cluster went down causing the customer impact.
​
Key Questions for Retro (enter as Problem Statements below): 
​
Q: 1. Tracer - Did we use Tracer? - No As of now tracer is not installed in the Commerce cloud environm
-
-
Site Reliability
-
8/24/2025 - 8/30/2025(1)
P1(1)
PRB-0028619(1)
SDB Performance
SDB
Analysis Complete
Performance degradation (general)
feature not included
PRB Retrospective | SEV-1 | 08/27/2025 | USA728 - APTs and Connpools

[THIS PRB IS MANAGED BY QUIP2GUS]
feature not included
8/28/2025
9/9/2025, 9:01 AM
79876927
W-19436758
User Experience: The customer experience during the incident was one of performance degradation and intermittent service issues. Users encountered: High APTs (Average Page Times): Webpages and application functions were slow to load. Connection Pool Exh...
[THIS PRB IS MANAGED BY QUIP2GUS]
Any updates made in GUS will be overwritten! Update Quip doc: https://salesforce.quip.com/jykNAJH679ky
​
An ongoing cyclical incident is causing high APTs and login failures across the fleet, spec
The proximate cause of this incident was a regression in the OS patch that was applied to the SDB pods on August 13, 2025.
The regression was the underlying issue, this was compounded by application the performance degradation where application level ac...
https://salesforce.pagerduty.com/incidents/Q2ELZEQPMU6EU6
In the case of USA728, it self resolved during the incident as impact was cyclical in nature. The incident resolution involved implementing a rollback of the OS patch suspected to be causing the issue. Additional guardrails of applying CTC locks were pl
Proximate Cause: The proximate cause of this incident was a regression in the OS patch that was applied to the SDB pods on August 13, 2025. The regression was the underlying issue, this was compounded by application the performance degradation where appl
1) A kernel change introduced by AWS into the EKS AL2 AMI that is consumed by Salesforce which resulted in increased DB connection and connection tear down times thus impacting core app and SDB performance..
2) SF is the only organization impacted. A
1) Forge to Develop process of testing with SDB requirements as part of their release.
Proximate Cause: The proximate cause of this incident was a regression in the OS patch that was applied to the SDB pods on August 13, 2025. The regression was the underlying issue, this was compounded by application the performance degradation where appl
-
-
Maeve Coleman
-
8/31/2025 - 9/6/2025(1)
P2(1)
PRB-0028630(1)
CRM Database Sustaining Engineering
SDB
Analysis Complete
Performance degradation (general)
feature not included
PRB Retrospective | SEV-2 | 08/31/2025 | USA556 - Rowlocks and misc DB issues

[THIS PRB IS MANAGED BY QUIP2GUS]
feature not included
8/31/2025
9/14/2025, 5:00 PM
80036207
W-19466903
User Experience: Customers were seeing latency and rowlocks.. | Impact Quantification: Q: What #/% of users/customers/requests got impacted (aggregated across all infra units)?. A: .
[THIS PRB IS MANAGED BY QUIP2GUS]
Any updates made in GUS will be overwritten! Update Quip doc: https://salesforce.quip.com/0te5ASpEq1Su
​
Multiple customers on USA556 were seeing rowlocks and latency. DB team saw RPC latency as o
The primary DB node was in the AZ that the prior apps were in before the deploy. This was causing RPC latency and some row locking.
https://salesforce-internal.slack.com/archives/CDQ3U66E5/p1756667266371899
CDSE failed the primary DB node over to the correct AZ
​
Q: Explain in detail the Time to Fix (i.e key events from detection to resolution)
A: 
​
Q: Was a known mitigation for this incident? If yes, please describe it and can it be automat
Proximate Cause: The primary DB node was in the AZ that the prior apps were in before the deploy. This was causing RPC latency and some row locking.
​
Key Questions for Retro (enter as Problem Statements below): 
​
Q: - Why did the primary DB
1)  On August 31st, a log owner node, . sdb-b-43-2. , in USA556 became a "sick node" with intermittent spikes in CPU, APT, and database metrics, leading to an AZ failover that resolved the immediate impact. This issue was not attributed to cust
1)  . Alert for Frequent standby Lag on Specific Standby Node.
2) SDB Node disable feature from Podtap.
3) Add pipeline support for killing a specific node in WS cluster.
4) Spike on moving app traffic when we have a Sick Node.
Proximate Cause: The primary DB node was in the AZ that the prior apps were in before the deploy. This was causing RPC latency and some row locking.
​
Key Questions for Retro (enter as Problem Statements below): 
​
Q: - Why did the primary DB
-
-
Site Reliability
-

